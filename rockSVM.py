from sklearn.svm import SVC
from sklearn import preprocessing
import numpy as np
from utils import dataset
import os
import random

def main():
	path = os.getcwd() + '/dataset/nl_features_subset.csv'
	csv_reader = dataset.load_data(path)
	matrix = []
	for line in csv_reader:
		if "song" in line or "genre" in line or len(line) == 0:
			# Ignore the head of the csv and any empty lines
			continue
		
		currentSong = []
		for i in range(1, len(line)):
			# check to see if we are working with the genre
			if i == 1:
				if line[i] == "Rock":
					# 1 is rock
					currentSong.append(1)
				else:
					# 0 is not rock
					currentSong.append(0)
			else:
				currentSong.append(float(line[i]))

		matrix.append(currentSong)

	# Shuffle the matrix before separating into the feature matrix and the classification vector
	random.shuffle(matrix)


	x = []
	y = []
	# Separate the matrix into the feature matrix and the classification vector
	for line in matrix:
		# TODO: Allow for different permutations of the data set to be ran

		x.append([float(line[1]), float(line[2]), float(line[3]), float(line[4]), float(line[5]), float(line[6]), float(line[7]), float(line[8]), float(line[9]), float(line[10]), float(line[11]), float(line[12]), float(line[13]), float(line[14]), float(line[15]), float(line[16]), float(line[17]), float(line[18])])
		# x.append([float(line[1]), float(line[2]), float(line[3]), float(line[4])])
		y.append(int(line[0]))
	x = np.asarray(x)
	y = np.asarray(y)

	# Scale the data matrix
	x_scaled = preprocessing.scale(x)

	# Next I would need some subset of the data matrix and classes to be used for training and testing
	
	# Note: support vector machine algorithms are not scale invariant, so it is highly recommended to scale your data
	# For example, scale each attribute on the inmput vector X to [0,1] or [-1,1]
	# Note: the same scaling must be applied to the test vector to obtain meaningful results
	
	# to be used for training
	x_train = x_scaled[:800]
	y_train = y[:800]

	# to be used for testing
	x_test = x_scaled[800:]
	y_test = y[800:]


	clf = SVC(C = 1, kernel = "linear")
	clf.fit(x_train, y_train)
	
	# Individually test each of the points in the testing set
	answers = []
	for i in range(800,999):
		next = i+1
		answer = clf.predict(x_scaled[i: next])
		answers.append(answer[0])

	print("Answers generated by the SVM, length =", len(answers))
	print(answers)

	print("Answers defined by the dataset, length = ", len(y_test))
	print(y_test)

	correct = 0
	for i in range(0, len(y_test)):
		if answers[i] == y_test[i]:
			correct += 1

	print("My Calculated percentage: " + str(correct/len(y_test)))
	scr = clf.score(x_test, y_test)
	print("The percentage calculated by sci-kit Learn: " + str(scr))
	# for k in ['linear', 'sigmoid', 'rbf']:
	# 	for i in range(1,100):

			# Setting C: c is 1 by default, and it's a reasonable default choice. If you have a lot of noisy observations, you should decrease it
			# it corresponds to regularize more the estimation
	# 		slack = float(i) / 10.0
	# 		clf = SVC(C = slack, kernel = k)

	# 		clf.fit(x_train, y_train)
	# 		answer = clf.predict(x_scaled[801:802])
	# 		print("Expected Answer: " + str(y[801:802]) + " Predicted answer",answer[0])
	# 		scr = clf.score(x_test, y_test)
	# 		print("\t",k, slack, scr, clf.support_vectors_.shape)

	


if __name__ == "__main__":
	main()
